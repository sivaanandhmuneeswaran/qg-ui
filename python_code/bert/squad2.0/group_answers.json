[{"index":2,"original_sentence":"existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects . this limitation severely hinders the use of these models in real world applications dealing with images in the wild . we address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time , without re-training . our method uses constrained beam search to force the inclusion of selected tag words in the output , and fixed , pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words . using this approach we achieve state of the art results for out-of-domain captioning on mscoco -LRB- and improved results for in-domain captioning -RRB- . perhaps surprisingly , our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm . we also show that we can significantly improve the quality of generated imagenet captions by leveraging ground-truth labels . ","tagged_sentence":"existing￨O_ANS image￨O_ANS captioning￨O_ANS models￨O_ANS do￨O_ANS not￨O_ANS generalize￨O_ANS well￨O_ANS to￨O_ANS out-of-domain￨O_ANS images￨O_ANS containing￨O_ANS novel￨O_ANS scenes￨O_ANS or￨O_ANS objects￨O_ANS .￨O_ANS this￨O_ANS limitation￨O_ANS severely￨O_ANS hinders￨O_ANS the￨O_ANS use￨O_ANS of￨O_ANS these￨O_ANS models￨O_ANS in￨O_ANS real￨O_ANS world￨O_ANS applications￨O_ANS dealing￨O_ANS with￨O_ANS images￨O_ANS in￨O_ANS the￨O_ANS wild￨O_ANS .￨O_ANS we￨O_ANS address￨O_ANS this￨O_ANS problem￨O_ANS using￨O_ANS a￨O_ANS flexible￨O_ANS approach￨O_ANS that￨O_ANS enables￨O_ANS existing￨O_ANS deep￨O_ANS captioning￨O_ANS architectures￨O_ANS to￨O_ANS take￨O_ANS advantage￨O_ANS of￨O_ANS image￨O_ANS taggers￨O_ANS at￨O_ANS test￨O_ANS time￨O_ANS ,￨O_ANS without￨O_ANS re-training￨O_ANS .￨O_ANS our￨O_ANS method￨O_ANS uses￨O_ANS constrained￨O_ANS beam￨O_ANS search￨O_ANS to￨O_ANS force￨O_ANS the￨O_ANS inclusion￨O_ANS of￨O_ANS selected￨O_ANS tag￨O_ANS words￨O_ANS in￨O_ANS the￨O_ANS output￨O_ANS ,￨O_ANS and￨O_ANS fixed￨O_ANS ,￨O_ANS pretrained￨O_ANS word￨B_ANS embeddings￨I_ANS to￨O_ANS facilitate￨O_ANS vocabulary￨O_ANS expansion￨O_ANS to￨O_ANS previously￨O_ANS unseen￨O_ANS tag￨O_ANS words￨O_ANS .￨O_ANS using￨O_ANS this￨O_ANS approach￨O_ANS we￨O_ANS achieve￨O_ANS state￨O_ANS of￨O_ANS the￨O_ANS art￨O_ANS results￨O_ANS for￨O_ANS out-of-domain￨O_ANS captioning￨O_ANS on￨O_ANS mscoco￨O_ANS -LRB-￨O_ANS and￨O_ANS improved￨O_ANS results￨O_ANS for￨O_ANS in-domain￨O_ANS captioning￨O_ANS -RRB-￨O_ANS .￨O_ANS perhaps￨O_ANS surprisingly￨O_ANS ,￨O_ANS our￨O_ANS results￨O_ANS significantly￨O_ANS outperform￨O_ANS approaches￨O_ANS that￨O_ANS incorporate￨O_ANS the￨O_ANS same￨O_ANS tag￨O_ANS predictions￨O_ANS into￨O_ANS the￨O_ANS learning￨O_ANS algorithm￨O_ANS .￨O_ANS we￨O_ANS also￨O_ANS show￨O_ANS that￨O_ANS we￨O_ANS can￨O_ANS significantly￨O_ANS improve￨O_ANS the￨O_ANS quality￨O_ANS of￨O_ANS generated￨O_ANS imagenet￨O_ANS captions￨O_ANS by￨O_ANS leveraging￨O_ANS ground-truth￨O_ANS labels￨O_ANS .￨O_ANS ","answer":"word embeddings","question":["What does pretrained stand for ?","What is pretrained ?","What does re-training stand for ?","What is the pretrained ?","What is the term for pretrained ?"],"score":[-2.3564553260803223,-3.8269970417022705,-4.229936122894287,-5.298074722290039,-5.689377307891846]}]
